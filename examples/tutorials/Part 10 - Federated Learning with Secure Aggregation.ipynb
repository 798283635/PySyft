{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Federated Learning with Encrypted Gradient Aggregation\n",
    "\n",
    "In the last few sections, we've been learning about encrypted computation by building several simple programs. In this section, we're going to return to the [Federated Learning Demo of Part 4](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%204%20-%20Federated%20Learning%20via%20Trusted%20Aggregator.ipynb), where we had a \"trusted aggregator\" who was responsible for averaging the model updates from multiple workers.\n",
    "\n",
    "We will now use our new tools for encrypted computation to remove this trusted aggregator because it is less than ideal as it assumes that we can find someone trustworthy enough to have access to this sensitive information. This is not always the case.\n",
    "\n",
    "Thus, in this notebook, we will show how one can use Secure Multi-Party Computation to perform secure aggregation such that we don't need a \"trusted aggregator\".\n",
    "\n",
    "Authors:\n",
    "- Theo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel)\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Normal Federated Learning\n",
    "\n",
    "First, here is some code which performs classic federated learning on the Boston Housing Dataset. This section of code is broken into several sections.\n",
    "\n",
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/BostonHousing/boston_housing.pickle','rb') as f:\n",
    "    ((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so we don't standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooking PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "james = sy.VirtualWorker(hook, id=\"james\")\n",
    "\n",
    "workers = [bob, alice]\n",
    "n_workers = len(workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Send data to the workers** <br>\n",
    "Usually they would already have it, this is just for demo purposes that we send it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(workers[batch_idx % len(workers)])\n",
    "    target = target.send(workers[batch_idx % len(workers)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/51 (0%)]\tLoss: 499.737000\n",
      "Train Epoch: 1 [80/51 (20%)]\tLoss: 444.433716\n",
      "Train Epoch: 1 [160/51 (39%)]\tLoss: 332.781464\n",
      "Train Epoch: 1 [240/51 (59%)]\tLoss: 133.071625\n",
      "Train Epoch: 1 [320/51 (78%)]\tLoss: 202.246490\n",
      "Train Epoch: 1 [200/51 (98%)]\tLoss: 16.212292\n",
      "Train Epoch: 2 [0/51 (0%)]\tLoss: 43.216457\n",
      "Train Epoch: 2 [80/51 (20%)]\tLoss: 8.341543\n",
      "Train Epoch: 2 [160/51 (39%)]\tLoss: 21.476635\n",
      "Train Epoch: 2 [240/51 (59%)]\tLoss: 24.701017\n",
      "Train Epoch: 2 [320/51 (78%)]\tLoss: 135.487076\n",
      "Train Epoch: 2 [200/51 (98%)]\tLoss: 16.858927\n",
      "Train Epoch: 3 [0/51 (0%)]\tLoss: 31.058380\n",
      "Train Epoch: 3 [80/51 (20%)]\tLoss: 6.284317\n",
      "Train Epoch: 3 [160/51 (39%)]\tLoss: 16.468424\n",
      "Train Epoch: 3 [240/51 (59%)]\tLoss: 20.231108\n",
      "Train Epoch: 3 [320/51 (78%)]\tLoss: 109.964325\n",
      "Train Epoch: 3 [200/51 (98%)]\tLoss: 14.596968\n",
      "Train Epoch: 4 [0/51 (0%)]\tLoss: 31.536034\n",
      "Train Epoch: 4 [80/51 (20%)]\tLoss: 6.582750\n",
      "Train Epoch: 4 [160/51 (39%)]\tLoss: 14.647825\n",
      "Train Epoch: 4 [240/51 (59%)]\tLoss: 15.352840\n",
      "Train Epoch: 4 [320/51 (78%)]\tLoss: 90.883629\n",
      "Train Epoch: 4 [200/51 (98%)]\tLoss: 13.207216\n",
      "Train Epoch: 5 [0/51 (0%)]\tLoss: 31.258961\n",
      "Train Epoch: 5 [80/51 (20%)]\tLoss: 7.210757\n",
      "Train Epoch: 5 [160/51 (39%)]\tLoss: 13.199806\n",
      "Train Epoch: 5 [240/51 (59%)]\tLoss: 11.943950\n",
      "Train Epoch: 5 [320/51 (78%)]\tLoss: 78.488693\n",
      "Train Epoch: 5 [200/51 (98%)]\tLoss: 12.271082\n",
      "Train Epoch: 6 [0/51 (0%)]\tLoss: 29.944611\n",
      "Train Epoch: 6 [80/51 (20%)]\tLoss: 7.750119\n",
      "Train Epoch: 6 [160/51 (39%)]\tLoss: 11.655147\n",
      "Train Epoch: 6 [240/51 (59%)]\tLoss: 10.470256\n",
      "Train Epoch: 6 [320/51 (78%)]\tLoss: 70.250122\n",
      "Train Epoch: 6 [200/51 (98%)]\tLoss: 11.836300\n",
      "Train Epoch: 7 [0/51 (0%)]\tLoss: 29.140284\n",
      "Train Epoch: 7 [80/51 (20%)]\tLoss: 7.879151\n",
      "Train Epoch: 7 [160/51 (39%)]\tLoss: 9.965439\n",
      "Train Epoch: 7 [240/51 (59%)]\tLoss: 9.868066\n",
      "Train Epoch: 7 [320/51 (78%)]\tLoss: 64.482697\n",
      "Train Epoch: 7 [200/51 (98%)]\tLoss: 11.616804\n",
      "Train Epoch: 8 [0/51 (0%)]\tLoss: 28.429161\n",
      "Train Epoch: 8 [80/51 (20%)]\tLoss: 8.152041\n",
      "Train Epoch: 8 [160/51 (39%)]\tLoss: 8.601869\n",
      "Train Epoch: 8 [240/51 (59%)]\tLoss: 9.669209\n",
      "Train Epoch: 8 [320/51 (78%)]\tLoss: 59.962410\n",
      "Train Epoch: 8 [200/51 (98%)]\tLoss: 11.714994\n",
      "Train Epoch: 9 [0/51 (0%)]\tLoss: 28.531370\n",
      "Train Epoch: 9 [80/51 (20%)]\tLoss: 8.310317\n",
      "Train Epoch: 9 [160/51 (39%)]\tLoss: 7.499838\n",
      "Train Epoch: 9 [240/51 (59%)]\tLoss: 9.452383\n",
      "Train Epoch: 9 [320/51 (78%)]\tLoss: 56.551590\n",
      "Train Epoch: 9 [200/51 (98%)]\tLoss: 11.322644\n",
      "Train Epoch: 10 [0/51 (0%)]\tLoss: 28.026127\n",
      "Train Epoch: 10 [80/51 (20%)]\tLoss: 8.640523\n",
      "Train Epoch: 10 [160/51 (39%)]\tLoss: 6.647422\n",
      "Train Epoch: 10 [240/51 (59%)]\tLoss: 9.245568\n",
      "Train Epoch: 10 [320/51 (78%)]\tLoss: 53.482040\n",
      "Train Epoch: 10 [200/51 (98%)]\tLoss: 10.839170\n",
      "Total 7.57 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 20.7677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Adding Encrypted Aggregation\n",
    "\n",
    "Now we're going to slightly modify this example to aggregate gradients using encryption. First, let's re-process our data and initialize a model for bob and alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = {worker.id: [] for worker in workers}\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    worker_idx = batch_idx % len(workers)\n",
    "    worker = workers[worker_idx]\n",
    "    data = data.send(worker)\n",
    "    target = target.send(worker)\n",
    "    train_distributed_dataset[worker.id].append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two main functionalities: the first one should at the beginning of an epoch send the same version of the model (the baseline) to all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one should, after each worker has done one epoch of training, perform the secure aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "And now that we know each step, we can put it all together into one training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = Net()\n",
    "models = [Net() for i in range(n_workers)]\n",
    "optimizers = [optim.SGD(models[i].parameters(), lr=args.lr) for i in range(n_workers)]\n",
    "\n",
    "def send_new_models(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for remote_model in models:\n",
    "            for new_param, remote_param in zip(local_model.parameters(), remote_model.parameters()):\n",
    "                worker = remote_param.location\n",
    "                remote_value = new_param.send(worker)\n",
    "                # Try this and if not do x_ptr * 0 + remote_value\n",
    "                remote_param.set_(remote_value)\n",
    "\n",
    "            \n",
    "def secure_aggregation(local_model, models):\n",
    "    with torch.no_grad():\n",
    "        for local_param, *remote_params in zip(*([local_model.parameters()] + [model.parameters() for model in models])):\n",
    "            param_stack = remote_params[0].copy().fix_prec().share(alice, bob, crypto_provider=james).get()\n",
    "            for remote_param in remote_params[1:]:\n",
    "                param_stack += remote_param.copy().fix_prec().share(alice, bob, crypto_provider=james).get()\n",
    "            param_stack /= len(remote_params)\n",
    "            param_stack = param_stack.get().float_prec()\n",
    "            local_param.set_(param_stack)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initial sending of the models\n",
    "    for model, optimizer, worker in zip(models, optimizers, workers):\n",
    "        model.send(worker)\n",
    "        \n",
    "    for epoch in range(args.epochs):\n",
    "        print(f'Epoch {epoch}')\n",
    "        \n",
    "        # 1. Send new version of the model\n",
    "        send_new_models(local_model, models)\n",
    "        \n",
    "        # 2. Train remotely the models\n",
    "        for i, worker in enumerate(workers):\n",
    "            model = models[i]\n",
    "            optimizer = optimizers[i]\n",
    "            dataloader = train_distributed_dataset[worker.id]\n",
    "            for (data, target) in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(data)\n",
    "                loss = F.mse_loss(pred.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # 3. Secure aggregation of the updated models\n",
    "        secure_aggregation(local_model, models)\n",
    "        \n",
    "    \n",
    "    # optional:\n",
    "    for model in models:\n",
    "        model.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Epoch: 0 [0/26 (0%)]\tLoss: 733.437378\n",
      "Train Epoch: 0 [80/26 (38%)]\tLoss: 782.898132\n",
      "Train Epoch: 0 [160/26 (77%)]\tLoss: 505.647644\n",
      "Train Epoch: 0 [0/25 (0%)]\tLoss: 254.449570\n",
      "Train Epoch: 0 [80/25 (40%)]\tLoss: 548.081482\n",
      "Train Epoch: 0 [160/25 (80%)]\tLoss: 242.105713\n",
      "Epoch 1\n",
      "Train Epoch: 1 [0/26 (0%)]\tLoss: 308.886322\n",
      "Train Epoch: 1 [80/26 (38%)]\tLoss: 268.282837\n",
      "Train Epoch: 1 [160/26 (77%)]\tLoss: 134.689972\n",
      "Train Epoch: 1 [0/25 (0%)]\tLoss: 54.217743\n",
      "Train Epoch: 1 [80/25 (40%)]\tLoss: 68.030266\n",
      "Train Epoch: 1 [160/25 (80%)]\tLoss: 17.645161\n",
      "Epoch 2\n",
      "Train Epoch: 2 [0/26 (0%)]\tLoss: 18.750498\n",
      "Train Epoch: 2 [80/26 (38%)]\tLoss: 143.332947\n",
      "Train Epoch: 2 [160/26 (77%)]\tLoss: 85.270027\n",
      "Train Epoch: 2 [0/25 (0%)]\tLoss: 35.880081\n",
      "Train Epoch: 2 [80/25 (40%)]\tLoss: 26.492050\n",
      "Train Epoch: 2 [160/25 (80%)]\tLoss: 22.781715\n",
      "Epoch 3\n",
      "Train Epoch: 3 [0/26 (0%)]\tLoss: 19.033188\n",
      "Train Epoch: 3 [80/26 (38%)]\tLoss: 123.948288\n",
      "Train Epoch: 3 [160/26 (77%)]\tLoss: 62.106342\n",
      "Train Epoch: 3 [0/25 (0%)]\tLoss: 18.531328\n",
      "Train Epoch: 3 [80/25 (40%)]\tLoss: 22.897598\n",
      "Train Epoch: 3 [160/25 (80%)]\tLoss: 24.166519\n",
      "Epoch 4\n",
      "Train Epoch: 4 [0/26 (0%)]\tLoss: 14.370845\n",
      "Train Epoch: 4 [80/26 (38%)]\tLoss: 112.280136\n",
      "Train Epoch: 4 [160/26 (77%)]\tLoss: 46.475597\n",
      "Train Epoch: 4 [0/25 (0%)]\tLoss: 11.897014\n",
      "Train Epoch: 4 [80/25 (40%)]\tLoss: 22.463121\n",
      "Train Epoch: 4 [160/25 (80%)]\tLoss: 23.906925\n",
      "Epoch 5\n",
      "Train Epoch: 5 [0/26 (0%)]\tLoss: 11.136672\n",
      "Train Epoch: 5 [80/26 (38%)]\tLoss: 104.639374\n",
      "Train Epoch: 5 [160/26 (77%)]\tLoss: 36.874786\n",
      "Train Epoch: 5 [0/25 (0%)]\tLoss: 8.934638\n",
      "Train Epoch: 5 [80/25 (40%)]\tLoss: 21.992821\n",
      "Train Epoch: 5 [160/25 (80%)]\tLoss: 22.915810\n",
      "Epoch 6\n",
      "Train Epoch: 6 [0/26 (0%)]\tLoss: 9.057338\n",
      "Train Epoch: 6 [80/26 (38%)]\tLoss: 99.302490\n",
      "Train Epoch: 6 [160/26 (77%)]\tLoss: 30.488752\n",
      "Train Epoch: 6 [0/25 (0%)]\tLoss: 7.348716\n",
      "Train Epoch: 6 [80/25 (40%)]\tLoss: 21.140867\n",
      "Train Epoch: 6 [160/25 (80%)]\tLoss: 21.909073\n",
      "Epoch 7\n",
      "Train Epoch: 7 [0/26 (0%)]\tLoss: 7.787767\n",
      "Train Epoch: 7 [80/26 (38%)]\tLoss: 94.874809\n",
      "Train Epoch: 7 [160/26 (77%)]\tLoss: 26.014053\n",
      "Train Epoch: 7 [0/25 (0%)]\tLoss: 6.320065\n",
      "Train Epoch: 7 [80/25 (40%)]\tLoss: 19.972198\n",
      "Train Epoch: 7 [160/25 (80%)]\tLoss: 21.037186\n",
      "Epoch 8\n",
      "Train Epoch: 8 [0/26 (0%)]\tLoss: 6.912008\n",
      "Train Epoch: 8 [80/26 (38%)]\tLoss: 91.429825\n",
      "Train Epoch: 8 [160/26 (77%)]\tLoss: 22.428268\n",
      "Train Epoch: 8 [0/25 (0%)]\tLoss: 5.598907\n",
      "Train Epoch: 8 [80/25 (40%)]\tLoss: 18.821087\n",
      "Train Epoch: 8 [160/25 (80%)]\tLoss: 20.379240\n",
      "Epoch 9\n",
      "Train Epoch: 9 [0/26 (0%)]\tLoss: 6.418332\n",
      "Train Epoch: 9 [80/26 (38%)]\tLoss: 88.449699\n",
      "Train Epoch: 9 [160/26 (77%)]\tLoss: 19.387663\n",
      "Train Epoch: 9 [0/25 (0%)]\tLoss: 5.037199\n",
      "Train Epoch: 9 [80/25 (40%)]\tLoss: 17.842821\n",
      "Train Epoch: 9 [160/25 (80%)]\tLoss: 19.900627\n",
      "Test set: Average loss: 17.8664\n",
      "\n",
      "Total 4.83 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "train()\n",
    "test()\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on Github\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft Github Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for github issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
